#+TITLE: Using MCMCglmm to implement lme4-like Bayesian mixed models
#+AUTHOR: Titus von der Malsburg, Meilin Zhan
#+EMAIL: malsburg@ucsd.edu, mezhan@mail.ucsd.edu
#+DATE: 11/06/2015

* TODO																														 :noexport:
- Use ~effectiveSize~ from the coda package.
- Mention other criteria for determining convergence (Heidelberg & Welch convergence diagnostic, Geweke’s convergence diagnostic, Raftery and Lewis diagnostic).

* Abstract

The lme4 package for fitting (generalized) linear mixed-effects models has proven to be a powerful tool for analyzing data from experimental studies.  However, most people encounter situations in which lme4 does not provide answers because the model-fitting process fails to converge.  This can be the case when the model is too complex to be supported by the data (c.f., Bates, Kliegl, Vasishth, Bayen, 2015).  Bayesian implementations of mixed-effects models can help in these situations because mild priors on the random effects parameters can be used to constrain the search space.  MCMCglmm is a package for fitting Bayesian mixed models in R and although its use is roughly similar to lme4’s there are some additional complexities that the user has to deal with.  This tutorial aims to get you started with MCMCglmm and shows how the Bayesian analogue of an lme4 model can be implemented with MCMCglmm.  Note that this text is not supposed to be an introduction to Bayesian mixed models or MCMC sampling more generally.  We rather assume that you are at least vaguely familiar with the basic ideas behind these concepts.

* Overview

To illustrate the use of MCMCglmm, we will analyze a data set from a language production experiment.  In this experiment, the participants were presented with visual scenes that they had to describe.  The aim of this study was to determine the factors that govern the participants use of pronouns in their descriptions.

We first attempt a model using lme4 and we will see how and why this fails.  Then we implement an analogous model using MCMCglmm.  MCMCglmm does not perform automatic convergence checks like lme4 does, which means that we have to handle this ourselves.  Since there are no definitive criteria for deciding whether the model has converged, we have to make use of a number of heuristics.  We first use some vague heuristics and then a somewhat more principled criterion to check convergence (Gelman, Rubin, 1992).  Finally, we evaluate the converged model with respect to the research question.

* Design of the study and data

#+BEGIN_SRC R :session *R* :exports none
setwd("/home/malsburg/Documents/Uni/Workshops/201511_MCMCglmm/MCMCglmm-intro")
load("data/models.Rda")
#+END_SRC

#+BEGIN_SRC R :session *R* :exports both :results output
d <- read.table("data/data.tsv", sep="\t", head=T)

head(d)
#+END_SRC

#+RESULTS:
:   subject item  a  b  c p
: 1       1    1 -1  1 -1 0
: 2       1    3  1 -1 -1 1
: 3       1    5 -1  1 -1 1
: 4       1    7  1 -1 -1 1
: 5       1    9 -1  1 -1 0
: 6       1   11 -1 -1 -1 0

#+BEGIN_SRC R :session *R* :exports both :results output
summary(d)
#+END_SRC

#+RESULTS:
:     subject           item             a                 b                  c                 p        
:  Min.   : 1.00   Min.   : 1.00   Min.   :-1.0000   Min.   :-1.00000   Min.   :-1.0000   Min.   :0.000  
:  1st Qu.:19.00   1st Qu.: 9.00   1st Qu.:-1.0000   1st Qu.:-1.00000   1st Qu.:-1.0000   1st Qu.:0.000  
:  Median :36.00   Median :18.00   Median : 1.0000   Median :-1.00000   Median :-1.0000   Median :1.000  
:  Mean   :35.89   Mean   :18.35   Mean   : 0.1421   Mean   :-0.02338   Mean   :-0.1241   Mean   :0.571  
:  3rd Qu.:54.00   3rd Qu.:27.25   3rd Qu.: 1.0000   3rd Qu.: 1.00000   3rd Qu.: 1.0000   3rd Qu.:1.000  
:  Max.   :71.00   Max.   :36.00   Max.   : 1.0000   Max.   : 1.00000   Max.   : 1.0000   Max.   :1.000

The experiment had a 2×2×2 design and the factors were ~a~, ~b~, and ~c~.  (The description of the experiment is censored because the study is not yet published.  We might add more details once that has happened.)  The factors ~a~ and ~b~ were within-item and ~c~ was between-items.  All three factors are coded using -1 and 1.  There were 71 subjects and 36 items, and each subject saw one version of each item.  The dependent variable indicates for each trial whether or not the participant used a pronoun in his description of a visual scene (1 if a pronoun was used, 0 otherwise). 

Proportions of pronoun responses per cell of the design:

#+BEGIN_SRC R :session *R* :exports both :results output
with(d, tapply(p, list(a, b, c), mean))
#+END_SRC

#+RESULTS:
#+begin_example
, , -1

          -1         1
-1 0.2647059 0.2352941
1  0.7234043 0.9577465

, , 1

         -1         1
-1 0.420000 0.2000000
1  0.875817 0.7829457
#+end_example

Looking at the contingency table, we see that some cells of the design had very few measurements.  This is expected because the factors were not experimentally controlled but properties of the utterance that the participants produced.

#+BEGIN_SRC R :session *R* :exports both :results output
with(d, table(a, b, c))
#+END_SRC

#+RESULTS:
#+begin_example
, , c = -1

    b
a     -1   1
  -1  34 238
  1  282  71

, , c = 1

    b
a     -1   1
  -1 100 105
  1  153 129
#+end_example

* Attempt to model the data with lme4

In principle, lme4 can deal with unbalanced data sets but the low number of data points in some cells of the design means that it is hard to estimate some of the effects.  In the present scenario, this is particularly a problem because the main question was whether the three-way interaction ~a:b:c~ (indicated in the following plot below) was significant or not.

#+BEGIN_SRC R :session *R* :results graphics :exports both :file plots/proportions_by_condition.png :width 400 :height 400 :res 100
plot(0:10, 0:10, t="n")
text(5, 5, labels="TODO: Add the plot.")
#+END_SRC

#+RESULTS:
[[file:plots/proportions_by_condition.png]]

We start with the maximal model justified by the design:

#+BEGIN_SRC R :session *R* :export code
library(lme4)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R
m1 <- glmer(p ~  (a + b + c)^3            +
                ((a + b + c)^3 | subject) + 
                ((a + b    )^2 | item),
            data=d, family="binomial")
#+END_SRC

A note on the formula notation used above: ~(a + b + c)^3~ is a little known alternative notation for ~a * b * c~.  So it gives us the main effects, the two-way interactions, and the three-way interaction.  The benefit of this notation is that it is more convenient during the design stage of the model when we often change the structure of the model.  For example if we want to exclude the three-way interaction, we can simply replace the 3 by a 2: ~(a + b + c)^2~.  So what the exponent says is up to which level we want to include interactions.

The model above is the most complex model that can be fit given the design.  The model has fixed effects terms for all three factors and all their interactions.  Following Barr, Levy, Scheepers, Tily (2013), there are also random slopes for all these factors.  The exception is ~c~ which was manipulated between items, so there can’t be item-slopes for that factor or any interaction in which this factor is involved.

The attempt to fit this model takes about 15 minutes on my machine and ultimately fails with one of the most colorful collections of warning messages I have ever seen from lme4:

#+BEGIN_EXAMPLE
Warning messages:
1: In commonArgs(par, fn, control, environment()) :
  maxfun < 10 * length(par)^2 is not recommended.
2: In optwrap(optimizer, devfun, start, rho$lower, control = control,  :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
3: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  unable to evaluate scaled gradient
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
#+END_EXAMPLE

Ben Bolker somewhere pointed out that the occurrence of a warning does not strictly imply that the model is degenerate, however, one of the above messages explicitly says that convergence failed and examining the model gives us good reasons to belief that: 

#+BEGIN_SRC R :session *R* :exports both :results output
summary(m1)
#+END_SRC

#+RESULTS:
#+begin_example
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: p ~ (np1 + active + ic1)^3 + ((np1 + active + ic1)^3 | subject) +      (np1 * active | item)
   Data: Exp3_free

     AIC      BIC   logLik deviance df.resid 
  1015.5   1286.2   -453.7    907.5     1058 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9524 -0.2471  0.0715  0.3325  3.3130 

Random effects:
 Groups  Name           Variance  Std.Dev. Corr                                     
 subject (Intercept)    30.410930 5.51461                                           
         np1            12.098270 3.47826   0.89                                    
         active         10.382867 3.22225   0.97  0.96                              
         ic1            15.047532 3.87911  -0.97 -0.96 -1.00                        
         np1:active     14.146457 3.76118   0.97  0.95  0.98 -0.99                  
         np1:ic1        11.313383 3.36354  -0.96 -0.96 -0.99  1.00 -1.00            
         active:ic1     14.433649 3.79916  -0.96 -0.97 -1.00  1.00 -0.98  0.99      
         np1:active:ic1 15.275932 3.90844  -0.95 -0.98 -0.99  0.99 -0.98  0.99  1.00
 item    (Intercept)     0.079619 0.28217                                           
         np1             0.086893 0.29478  -1.00                                    
         active          0.006393 0.07995  -1.00  1.00                              
         np1:active      0.026452 0.16264  -0.99  0.99  0.97                        
Number of obs: 1112, groups:  subject, 71; item, 36

Fixed effects:
               Estimate Std. Error z value Pr(>|z|)    
(Intercept)       3.764      1.484   2.537 0.011189 *  
np1               5.260      1.403   3.750 0.000177 ***
active            2.953      1.371   2.154 0.031274 *  
ic1              -3.024      1.393  -2.171 0.029926 *  
np1:active        3.756      1.387   2.707 0.006783 ** 
np1:ic1          -3.296      1.378  -2.392 0.016774 *  
active:ic1       -3.621      1.407  -2.574 0.010048 *  
np1:active:ic1   -3.642      1.410  -2.583 0.009787 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Correlation of Fixed Effects:
            (Intr) np1    active ic1    np1:ct np1:c1 actv:1
np1          0.957                                          
active       0.967  0.989                                   
ic1         -0.980 -0.980 -0.986                            
np1:active   0.980  0.980  0.983 -0.992                     
np1:ic1     -0.968 -0.988 -0.993  0.984 -0.986              
active:ic1  -0.974 -0.990 -0.991  0.987 -0.985  0.993       
np1:actv:c1 -0.978 -0.984 -0.984  0.992 -0.990  0.986  0.987
convergence code: 0
unable to evaluate scaled gradient
Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
failure to converge in 10000 evaluations

Warning messages:
1: In vcov.merMod(object, use.hessian = use.hessian) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite or contains NA values: falling back to var-cov estimated from RX
2: In vcov.merMod(object, correlation = correlation, sigm = sig) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite or contains NA values: falling back to var-cov estimated from RX
#+end_example

The estimates of the correlations of random effects are all close to -1 or 1 and all fixed effects and interactions are highly significant, both is highly implausible.  The standard thing to do in this situation is to simplify the model until it converges.  According to Barr et al., one constraint in doing do is that the random slopes for the effect of interest (the effect about which we want to make inferences, in this case the three-way interaction ~a:b:c~) need to be in the model, otherwise there may be an inflated chance of false positive effects.  Under this constraint, the simplest possible model is the following:

#+BEGIN_SRC R
m2 <- glmer(p ~ (a + b + c)^3 +
                (0 + a : b : c |subject) + 
                (0 + a : b     |item),
            data=d, family="binomial")
#+END_SRC

#+BEGIN_EXAMPLE
Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  unable to evaluate scaled gradient
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
#+END_EXAMPLE

Unfortunately, the model also fails to converge as do all other variations that we tried, specifically the intercepts-only model.  The model fit (see below) looks more reasonable this time but we clearly can’t rely on this model.  Since we are already using the simplest permissible model, we reached the end of the line of what we can do with lme4.

#+BEGIN_SRC R :session *R* :exports results :results output
summary(m2)
#+END_SRC

#+RESULTS:
#+begin_example
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: p ~ (np1 + active + ic1)^3 + (0 + np1:active:ic1 | subject) +      (0 + np1:active | item)
   Data: Exp3_free

     AIC      BIC   logLik deviance df.resid 
  1133.9   1184.0   -556.9   1113.9     1102 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-8.4530 -0.5253  0.2503  0.5369  4.1687 

Random effects:
 Groups  Name           Variance  Std.Dev. 
 subject np1:active:ic1 5.498e-01 0.7415049
 item    np1:active     2.526e-07 0.0005026
Number of obs: 1112, groups:  subject, 71; item, 36

Fixed effects:
                Estimate Std. Error z value Pr(>|z|)    
(Intercept)     0.444294   0.113699   3.908 9.32e-05 ***
np1             1.576301   0.118933  13.254  < 2e-16 ***
active          0.062480   0.112740   0.554  0.57945    
ic1            -0.008851   0.113678  -0.078  0.93794    
np1:active      0.360923   0.111885   3.226  0.00126 ** 
np1:ic1        -0.196345   0.112047  -1.752  0.07972 .  
active:ic1     -0.537264   0.114899  -4.676 2.93e-06 ***
np1:active:ic1 -0.209187   0.142544  -1.468  0.14223    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Correlation of Fixed Effects:
            (Intr) np1    active ic1    np1:ct np1:c1 actv:1
np1          0.235                                          
active       0.253  0.545                                   
ic1         -0.411 -0.194 -0.232                            
np1:active   0.563  0.256  0.234 -0.631                     
np1:ic1     -0.231 -0.428 -0.641  0.222 -0.246              
active:ic1  -0.248 -0.640 -0.431  0.237 -0.234  0.565       
np1:actv:c1 -0.492 -0.166 -0.176  0.443 -0.338  0.192  0.170
convergence code: 0
unable to evaluate scaled gradient
Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
#+end_example

As indicated above, Bayesian mixed models may help in this situation.  However, before we embark on an Bayesian adventure, we should consider a much simpler solution: the t-test!  The t-test can be used to test whether the difference between two sets of data is significant.  Since a three-way interaction is nothing else but a difference of differences of differences, the t-test would be perfectly appropriate.  The appeal of this is of course that the t-test is simple and relatively fool-proof, plus there is no risk of convergence errors.  The approach would be to calculate the differences of differences on a by-subject basis, and to conduct a paired t-test with these values.  However, there is one catch.  Our data are so sparse that the vast majority of subjects (62 out of 71) do not have measurements in all eight cells of the design.  Hence we can calculate the necessary difference values only for a tiny subset of the subjects. 

* Using MCMCglmm

The specification of a model in MCMCglmm is relatively similar to lme4.  The are two main differences.  First, since MCMCglmm is Bayesian, we need to add a specification of the priors.  Second, we have to set some parameters for the model fitting process manually.

Below you see the definition of the maximal model corresponding to the first lme4 model above (~m1~). 

#+BEGIN_SRC R
library(MCMCglmm)
set.seed(14)

prior.m3 <- list(
  R=list(V=1, n=1),
  G=list(G1=list(V        = diag(8),
                 n        = 8,
                 alpha.mu = rep(0, 8),
                 alpha.V  = diag(8)*25^2),
         G2=list(V        = diag(4),
                 n        = 4,
                 alpha.mu = rep(0, 4),
                 alpha.V  = diag(4)*25^2)))

m3 <- MCMCglmm(p ~ (a + b + c)^3,
                 ~ us(1 + (a + b + c)^3):subject +
                   us(1 + (a + b    )^2):item,
               data   = d,
               family = "categorical",
               prior  = prior.m3,
               thin   = 1,
               burnin = 3000,
               nitt   = 4000)
#+END_SRC

The variable ~prior.m3~ contains the specification of the priors.  Priors can be defined for the residuals, the fixed effects, and the random effects.  Here we only specify priors for the residuals (~R~) and the random effects (~G~).  The distribution used for the priors is the inverse-Wishart distribution, a probability distribution on covariance matrices.  The univariate special case of the inverse-Wishart distribution is the inverse-gamma distribution.  This form is used as the prior for the variance of the residuals.  ~V~ is the scale matrix of the inverse-Wishart and equals 1 because we want the univariate case. ~n~ is the degrees of freedom parameter and is set to 1 which gives us the weakest possible prior.

~G1~ is the prior definition for the eight subject random effects. ~V~ is set to 8 because we have eight random effects for subjects (intercept, the three factors, their three two-way interactions, and one three-way interaction) and the covariance matrix therefore needs 8×8 entries.  Again, ~n~ is set to give us the weakest prior (the lower bound for ~n~ is the number of dimensions).  Further, we have parameters ~alpha.mu~ and ~alpha.V~.  These specify an additional prior which is used for parameter expansion, which is basically a trick to improve the rate of convergence.  All we care about is that the ~alpha.mu~ is a vector of as many zeros as there are random effects and that ~alpha.V~ is a n×n matrix with large numbers on the diagonal and n being the number of random effects.  See Hadfield (2010) and Hadfield’s course notes on MCMCglmm (included in the R package) for details.

~G2~ defines the prior for the by-item random effects and follows the same scheme.  The only differences is that we have only four item random effects instead of the eight for subjects (because ~c~ is constant within item).  In sum, these definitions give us mild priors for the residuals and random effects.

In MCMCglmm, the specification of the model structure is split into two parts.  The fixed-effects part looks exactly as in lme4 (=p~(a+b+c)^3=).  The random-effects part is a little different.  lme4 by default assumes that we want a completely parameterized covariance matrix, that is that we want to estimate the variances of the random effects and all covariances.  MCMCglmm wants us to make this explicit.  The notation ~us(…)~ can use used to specify parameters for all variances and covariances, in other words it gives us the same that lme4 would do by default.  An alternative would be to use ~idh(…)~ which tells MCMCglmm to estimate parameters for the variances but not for the covariances.

Next, we need to specify the family of the dependent variable.  For the glmer model this is ~binomial~, but MCMCglmm uses ~categorical~ which can also be used for dependent variables with more than two levels.

Finally, we need to set some parameters that control the MCMC sampling process.  This process uses the data and the model specification to draw samples from the posterior distribution and as we collect more and more samples the shape of this distribution emerges more and more clearly.  Inferences are then made based on that approximation of the true distribution. 

There are three parameters that we need to set to control the sampling process: ~nitt~, ~burnin~, and ~thin~.  ~nitt~ is set to 4000 and defines how many samples we want to produce overall.  ~burnin~ is set to 3000 and defines the length (in samples) of the so-called burn-in period after which we start collecting samples.  The idea behind this is that the first samples may be influenced by the random starting point of the sampling process and may therefore not represent the true distribution.  Ideally, consecutive samples would be statistically independent, but that is rarely the case in practice.  Thinning can be used to reduce the resulting autocorrelation and is controlled by the parameter ~thin~. ~thin=n~ means that we want to keep every n-th sample.  Here we set ~thin~ to 1.  In sum, these parameter settings give us 1000 usable samples (4000 - 3000).

Below we see the posterior means and quantiles obtained with the above model.  The pattern of results looks qualitatively similar that in the glmer model but there are considerable numerical differences.  However, as mentioned earlier, MCMCglmm does not check convergence and these results may be unreliable.  Below we will examine the results more closely to determine whether we can trust them.
 
#+BEGIN_SRC R :session *R* :exports both :results output
summary(m3$Sol)
#+END_SRC

#+RESULTS:
#+begin_example

Iterations = 3001:4000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 1000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

               Mean     SD Naive SE Time-series SE
(Intercept)  1.3475 0.4189 0.013246        0.06731
a            3.1882 0.2967 0.009382        0.06020
b           -0.2202 0.2300 0.007275        0.06802
c            0.0577 0.2299 0.007271        0.05356
a:b          0.8467 0.3243 0.010257        0.13246
a:c         -0.2605 0.2454 0.007759        0.09630
b:c         -1.1221 0.2007 0.006348        0.03561
a:b:c       -0.9962 0.2921 0.009238        0.10529

2. Quantiles for each variable:

                2.5%     25%      50%      75%   97.5%
(Intercept)  0.52905  1.0558  1.35092  1.63646  2.2106
a            2.61218  2.9793  3.19866  3.40216  3.7413
b           -0.61128 -0.3816 -0.24456 -0.06253  0.2465
c           -0.33693 -0.1002  0.02712  0.19129  0.5865
a:b          0.01218  0.6840  0.88057  1.06400  1.3636
a:c         -0.71437 -0.4479 -0.25036 -0.07384  0.1743
b:c         -1.52459 -1.2596 -1.10782 -0.98058 -0.7350
a:b:c       -1.50290 -1.2142 -1.01716 -0.78711 -0.4160
#+end_example

* Plotting the samples

One way to get a sense of whether the samples drawn by MCMCglmm are an accurate representation of the true posterior is to plot them.  In the panels on the left, we see the traces of the parameters showing which values the parameters assumed throughout the sampling process; the index of the sample is on the x-axis (starting with 3000 because we discarded the first 3000 samples) and the value of the parameter in that sample is on the y-axis.  In the panels on the right, we see the distribution of the values that the parameters assumed over the course of the sampling process, i.e. the posterior.

#+BEGIN_SRC R :session *R* :results graphics :exports both :file plots/samples_1.png :width 800 :height 1000 :res 100
par(mfrow=c(8,2), mar=c(2,2,1,0))
plot(m3$Sol, auto.layout=F)
#+END_SRC

#+RESULTS:
[[file:plots/samples_1.png]]

There are a number of signals in these plots that suggest that our sample may not be good enough.  First, there is high autocorrelation, which means that samples tend to have similar parameter values as the directly preceding samples.  Second, the traces of the parameters are not /stationary/, which means that the sampling process dwells in one part of the parameter space and then suddenly visits other parts of the parameter space.  This can be observed at around 3900 samples where the trace of ~c~ suddenly moves to more positive values not visited before and the trace of ~a:b~ moves to more negative values.  Both taken together these properties suggest that our sample is not yet a good-enough approximation of the true posterior distribution.  Think about it this way: looking at these plots, is it likely that the density plots on the right would change if we would continue taking samples?  Yes, it is because there may be more sudden moves to other parts of the parameter space like that at around 3900.  Or the sampling process might dwell in the position at 4000 for a longer time leading to shift in the distributions.  For example the density plot of ~a:b~ has a long tail coming from the last ~100 samples and this tail might have gotten fatter if we hadn’t ended the sampling process at 4000.  As long as these density plots keep changing, the sampling process has not converged and we don’t have a stable posterior.  Ideally, what we would like to have is something like the following:

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/samples_2.png :width 800 :height 125 :res 60
set.seed(1)
par(mfrow=c(1,2), mar=c(2,2,1,0))
x <- rnorm(1000)
plot(3001:4000, x, t="l", main="Trace of x")
plot(density(x), main="Density of x")
#+END_SRC

#+RESULTS:
[[file:plots/samples_2.png]]

In this trace plot of random data, there is no autocorrelation of consecutive samples and the distribution of samples is stationary.  It is very likely that taking more samples wouldn’t shift the distribution substantially.  Hence, if we see a plot like this, we would be more confident that our posterior is a good approximation of the true posterior.

There are several things that we can do in order to improve our sample.  We can collect more samples until all parts of the parameter space have been visited approximately the right amount of times.  And we can try to reduce the autocorrelation of the samples in order to avoid that some parts of the parameter space are over-represented.

# Wiping the floor metaphor useful or not?

What thinning factor?  Plot of the autocorrelation function for each parameter.

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/autocorrelation_1.png :width 800 :height 600 :res 100
plot.acfs <- function(x) {
  n <- dim(x)[2]
  par(mfrow=c(ceiling(n/2),2), mar=c(2,2,3,0))
  for (i in 1:n) {
    acf(x[,i], lag.max=100, main=colnames(x)[i])
    grid()
  }
}
plot.acfs(m3$Sol)
#+END_SRC

#+RESULTS:
[[file:plots/autocorrelation_1.png]]

Use thinning factor of 20 to get rid of some of the autocorrelation:

#+BEGIN_SRC R
m4 <- MCMCglmm(p ~ (a + b + c)^3,
                 ~ us(1 + (a + b + c)^3):subject +
                   us(1 + (a + b    )^2):item,
               data   = d,
               family = "categorical",
               prior  = prior.m3,
               thin   = 20,
               burnin = 3000,
               nitt   = 23000)
#+END_SRC

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/samples_3.png :width 800 :height 400 :res 100
chain.plot <- function(x) {
  n <- dim(x)[2]
  par(mfrow=c(ceiling(n/2),2), mar=c(0,0.5,1,0.5))
  for (i in 1:n) {
    plot(as.numeric(x[,i]), t="l", main=colnames(x)[i], xaxt="n", yaxt="n")
  }
}
chain.plot(m4$Sol)
#+END_SRC

#+RESULTS:
[[file:plots/samples_3.png]]

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/autocorrelation_2.png :width 800 :height 600 :res 100
plot.acfs(m4$Sol)
#+END_SRC

#+RESULTS:
[[file:plots/autocorrelation_2.png]]

Ok, we need to simplify the model.  This model had only random intercepts and the random sloped for the effects of interest:

#+BEGIN_SRC R
prior.m5 <- list(
  R=list(V=1, n=1),
  G=list(G1=list(V        = diag(2),
                 n        = 2,
                 alpha.mu = rep(0, 2),
                 alpha.V  = diag(2)*25^2),
         G2=list(V        = diag(2),
                 n        = 2,
                 alpha.mu = rep(0, 2),
                 alpha.V  = diag(2)*25^2)))

m5 <- MCMCglmm(p ~ (a + b + c)^3,
                 ~ us(1 + a : b : c):subject +
                   us(1 + a : b    ):item,
               data   = d,
               family = "categorical",
               prior  = prior.m5,
               thin   = 1,             # No thinning!
               burnin = 3000,
               nitt   = 4000)
#+END_SRC

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/samples_4.png :width 800 :height 400 :res 100
chain.plot(m5$Sol)
#+END_SRC

#+RESULTS:
[[file:plots/samples_4.png]]

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/autocorrelation_3.png :width 800 :height 600 :res 100
plot.acfs(m5$Sol)
#+END_SRC

#+RESULTS:
[[file:plots/autocorrelation_3.png]]

Still too much autocorrelation but this time thinning may help:

#+BEGIN_SRC R
m6 <- MCMCglmm(p ~ (a + b + c)^3,
                 ~ us(1 + a : b : c):subject +
                   us(1 + a : b    ):item,
               data   = d,
               family = "categorical",
               prior  = prior2,
               thin   = 20,
               burnin = 3000,
               nitt   = 23000)
#+END_SRC

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/samples_5.png :width 800 :height 400 :res 100
chain.plot(m6$Sol)
#+END_SRC

#+RESULTS:
[[file:plots/samples_5.png]]

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/autocorrelation_4.png :width 800 :height 600 :res 100
plot.acfs(m6$Sol)
#+END_SRC

#+RESULTS:
[[file:plots/autocorrelation_4.png]]

Looks good but a more formal criterion would be nice.

* Gelman-Rubin criterion

Running multiple chains so we can calculate the Gelman-Rubin criterion:

#+BEGIN_SRC R
library(parallel)

ml <- mclapply(1:4, function(i) {
  MCMCglmm(p ~ (a + b + c)^3,
           random = ~us(1 + a : b : c):subject +
                     us(1 + a : b)      :item,
           data   = d,
           family = "categorical",
           prior  = prior2,
           thin   = 20,
           burnin = 3000,
           nitt   = 43000)
}, mc.cores=4)

ml <- lapply(ml, function(m) m$Sol)
ml <- do.call(mcmc.list, ml)
#+END_SRC

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/gelman_rubin.png :width 800 :height 600 :res 100
library(coda)

par(mfrow=c(4,2), mar=c(2,2,1,2))
gelman.plot(ml, auto.layout=F)
#+END_SRC

#+RESULTS:
[[file:plots/gelman_rubin.png]]

#+BEGIN_SRC R :session *R* :exports both :results output
gelman.diag(ml)
#+END_SRC

#+RESULTS:
#+begin_example
Potential scale reduction factors:

            Point est. Upper C.I.
(Intercept)       1.01       1.02
a                 1.01       1.02
b                 1.00       1.01
c                 1.01       1.03
a:b               1.01       1.03
a:c               1.00       1.01
b:c               1.00       1.00
a:b:c             1.01       1.04

Multivariate psrf

1.02
#+end_example

The chains are mixing:

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/samples_6.png :width 800 :height 1000 :res 100
par(mfrow=c(8,2), mar=c(2, 1, 1, 1))
plot(ml, ask=F, auto.layout=F)
#+END_SRC

#+RESULTS:
[[file:plots/samples_6.png]]


* Results

#+BEGIN_SRC R :session *R* :exports both :results output
summary(ml)
#+END_SRC

#+RESULTS:
#+begin_example

Iterations = 3001:42981
Thinning interval = 20 
Number of chains = 4 
Sample size per chain = 2000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

                Mean     SD Naive SE Time-series SE
(Intercept)  0.87581 0.3205 0.003583       0.005734
a            2.15269 0.1703 0.001905       0.005747
b           -0.13405 0.1549 0.001732       0.004207
c           -0.05516 0.1635 0.001828       0.004367
a:b          0.62410 0.1614 0.001804       0.004129
a:c         -0.09112 0.1524 0.001703       0.004033
b:c         -0.54022 0.1585 0.001772       0.004961
a:b:c       -0.38862 0.1678 0.001876       0.004331

2. Quantiles for each variable:

               2.5%     25%      50%      75%    97.5%
(Intercept)  0.2656  0.6522  0.87330  1.09195  1.51079
a            1.8376  2.0329  2.14534  2.26614  2.50369
b           -0.4252 -0.2424 -0.13610 -0.02909  0.17210
c           -0.3819 -0.1639 -0.05209  0.05814  0.25470
a:b          0.3135  0.5162  0.62068  0.73132  0.94144
a:c         -0.3883 -0.1931 -0.08977  0.01287  0.20831
b:c         -0.8705 -0.6417 -0.53590 -0.43160 -0.24013
a:b:c       -0.7389 -0.4974 -0.38380 -0.27406 -0.07854
#+end_example

Plot of the parameter estimates with 95% credible intervals:

#+BEGIN_SRC R :session *R* :exports both :results graphics :file plots/parameter_estimates.png :width 600 :height 300 :res 80
plot.estimates <- function(x) {
  if (class(x) != "summary.mcmc")
    x <- summary(x)
  n <- dim(x$statistics)[1]
  par(mar=c(2, 7, 4, 1))
  plot(x$statistics[,1], n:1,
       yaxt="n", ylab="",
       xlim=range(x$quantiles)*1.2,
       pch=19,
       main="Posterior means and 95% credible intervals")
  grid()
  axis(2, at=n:1, rownames(x$statistics), las=2)
  arrows(x$quantiles[,1], n:1, x$quantiles[,5], n:1, code=0)
  abline(v=0, lty=2)
}

plot.estimates(ml)
#+END_SRC

#+RESULTS:
[[file:plots/parameter_estimates.png]]

Yay, the three way interaction is significant! But note that we can't really evaluate other effects because the model doesn't have the corresponding random slopes.

* Summary
- Pros
  - more flexibility
  - more control
  - more transparent
  - more complex models possible
  - gives us credible intervals instead of confidence intervals
- Cons
  - requires more technical knowledge into the process
  - model fitting process perhaps more involved (but messing with lmer
    models is actually as much work but messier)

* References

- Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random
  effects structure for confirmatory hypothesis testing: Keep it
  maximal. Journal of Memory and Language, 68(3),
  255–278. http://dx.doi.org/10.1016/j.jml.2012.11.001
- Bates, D., Kliegl, R., Vasishth, S., & Baayen,
  H. (2015). Parsimonious mixed models. Manuscript published on arXiv.
  http://arxiv.org/abs/1506.04967
- Gelman, A., & Rubin, D. B. (1992). Inference from iterative
  simulation using multiple sequences. Statistical Science, 7(4),
  457–472.
- Hadfield, J. (2010). MCMC methods for multi-response generalized
  linear mixed models: the MCMCglmm R package. Journal of Statistical
  Software, 33(1), 1–22. http://dx.doi.org/10.18637/jss.v033.i02



